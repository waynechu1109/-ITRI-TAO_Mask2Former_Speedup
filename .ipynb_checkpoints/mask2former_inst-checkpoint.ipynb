{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance segmentation using TAO Mask2former\n",
    "\n",
    "[Masked-attention Mask Transformer for Universal Image Segmentation (Mask2former)](https://arxiv.org/pdf/2112.01527.pdf) is a high-quality Transformer-based segmentation framework for instance, semantic and panoptic segmentation. Mask2former takes RGB images as inputs and generates mask predictions and their associated categories.\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://d29g4g2dyqv443.cloudfront.net/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample prediction of Mask2former model\n",
    "<img align=\"center\" src=\"https://github.com/vpraveen-nv/model_card_images/blob/main/cv/notebook/common/mal_sample.jpg?raw=true\" width=\"960\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained model and train a Mask2former model on COCO dataset\n",
    "* Evaluate the trained model\n",
    "* Run inference with the trained model and visualize the result\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of Mask2former using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and download pretrained model](#head-2)\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate a trained model](#head-5)\n",
    "6. [Run inference](#head-6)\n",
    "7. [Deploy](#head-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/mask2former/`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOCAL_PROJECT_DIR=/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "%env LOCAL_PROJECT_DIR=/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former\n",
    "\n",
    "os.environ[\"HOST_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"HOST_RESULTS_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"mask2former\")\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/mask2former\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"HOST_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $HOST_DATA_DIR\n",
    "! mkdir -p $HOST_SPECS_DIR\n",
    "! mkdir -p $HOST_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "import os\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "tao_configs = {\n",
    "   \"Mounts\":[\n",
    "         # Mapping the Local project directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_DATA_DIR\"],\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_SPECS_DIR\"],\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": os.environ[\"HOST_RESULTS_DIR\"],\n",
    "           \"destination\": \"/results_inst\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         },\n",
    "        # \"user\": \"{}:{}\".format(os.getuid(), os.getgid()),\n",
    "        \"network\": \"host\"\n",
    "   }\n",
    "}\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(tao_configs, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Mounts\": [\n",
      "        {\n",
      "            \"source\": \"/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former\",\n",
      "            \"destination\": \"/workspace/tao-experiments\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/data\",\n",
      "            \"destination\": \"/data\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/specs\",\n",
      "            \"destination\": \"/specs\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/mask2former\",\n",
      "            \"destination\": \"/results_inst\"\n",
      "        }\n",
      "    ],\n",
      "    \"DockerOptions\": {\n",
      "        \"shm_size\": \"16G\",\n",
      "        \"ulimits\": {\n",
      "            \"memlock\": -1,\n",
      "            \"stack\": 67108864\n",
      "        },\n",
      "        \"network\": \"host\"\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.7, <=3.10.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (1.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-tao in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (5.5.1)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (2025.1.31)\n",
      "Requirement already satisfied: chardet==3.0.4 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (3.0.4)\n",
      "Requirement already satisfied: docker-pycreds==0.4.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (0.4.0)\n",
      "Requirement already satisfied: docker==4.3.1 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (4.3.1)\n",
      "Requirement already satisfied: idna==2.10 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (2.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (2.31.0)\n",
      "Requirement already satisfied: rich<14.0,>=13.6.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (13.9.4)\n",
      "Requirement already satisfied: six==1.15.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (1.15.0)\n",
      "Requirement already satisfied: tabulate<1.0,>=0.9.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.15 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (1.26.20)\n",
      "Requirement already satisfied: websocket-client==0.57.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from nvidia-tao) (0.57.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from requests==2.31.0->nvidia-tao) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from rich<14.0,>=13.6.0->nvidia-tao) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from rich<14.0,>=13.6.0->nvidia-tao) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from rich<14.0,>=13.6.0->nvidia-tao) (4.13.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/lidar/anaconda3/envs/taolauncher/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0,>=13.6.0->nvidia-tao) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "task_group: ['model', 'dataset', 'deploy']\n",
      "format_version: 3.0\n",
      "toolkit_version: 5.5.0\n",
      "published_date: 08/26/2024\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and download pretrained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the COCO dataset for the tutorial. The following script will download COCO dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ '[' -z /home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/data ']'\n",
      "+ UNZIP='unzip -nq'\n",
      "+ OUTPUT_DIR=/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/data/raw-data\n",
      "+ mkdir -p /home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/data/raw-data\n",
      "++ pwd\n",
      "+ CURRENT_DIR=/home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former\n",
      "+ cd /home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/data/raw-data\n",
      "+ BASE_IMAGE_URL=http://images.cocodataset.org/zips\n",
      "+ TRAIN_IMAGE_FILE=train2017.zip\n",
      "+ download_and_unzip http://images.cocodataset.org/zips train2017.zip\n",
      "+ local BASE_URL=http://images.cocodataset.org/zips\n",
      "+ local FILENAME=train2017.zip\n",
      "+ '[' '!' -f train2017.zip ']'\n",
      "++ pwd\n",
      "+ echo 'Downloading train2017.zip to /home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/data/raw-data'\n",
      "Downloading train2017.zip to /home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/data/raw-data\n",
      "+ wget -nd -c http://images.cocodataset.org/zips/train2017.zip\n",
      "--2025-05-08 14:31:10--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.115.57, 16.15.179.102, 16.15.178.33, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.115.57|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘train2017.zip’\n",
      "\n",
      "train2017.zip         9%[>                   ]   1.74G  12.3MB/s    eta 22m 46s^C\n"
     ]
    }
   ],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $HOST_DATA_DIR\n",
    "# Download the data\n",
    "!bash $HOST_SPECS_DIR/download_coco.sh $HOST_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8380\r\n",
      "drwxrwxr-x 2 lidar lidar    4096 Apr 22 13:49 annotations\r\n",
      "drwxr-xr-x 2 lidar lidar 4096000 Jun  8  2018 panoptic_train2017\r\n",
      "drwxr-xr-x 2 lidar lidar  167936 Jun 13  2018 panoptic_val2017\r\n",
      "drwxrwxr-x 2 lidar lidar 4120576 Aug 31  2017 train2017\r\n",
      "drwxrwxr-x 2 lidar lidar  167936 Aug 31  2017 val2017\r\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "!ls -l $HOST_DATA_DIR/raw-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will download the original Swin-Tiny model from GitHub. \n",
    " For more details about the model, please refer to https://github.com/microsoft/Swin-Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /home/lidar/tao_tutorials/notebooks/tao_launcher_starter_kit/mask2former/mask2former: Is a directory\n"
     ]
    }
   ],
   "source": [
    "# Download the pretrained model\n",
    "# !wget https://github.com/SwinTransformer/storage/releases/download/v1.0.8/swin_tiny_patch4_window7_224_22k.pth -o $LOCAL_PROJECT_DIR/mask2former\n",
    "# !wget https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/torchvision/R-50.pkl -o $LOCAL_PROJECT_DIR/mask2former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that model is downloaded into dir.\n",
      "total 1181412\n",
      "drwxr-xr-x 2 root  root       4096 Apr 23 13:50 evaluate\n",
      "drwxr-xr-x 2 root  root       4096 Apr 23 16:56 export\n",
      "drwxr-xr-x 2 root  root       4096 Apr 23 17:02 gen_trt_engine\n",
      "drwxr-xr-x 2 root  root       4096 Apr 23 16:52 inference\n",
      "-rw-rw-r-- 1 lidar lidar 102465227 May  8 14:50 R-50.pkl\n",
      "-rw-r--r-- 1 root  root       2041 Apr 25 22:23 status.json\n",
      "-rw-rw-r-- 1 lidar lidar 928819451 May  9 11:08 swin_large_patch4_window12_384_22k.pth\n",
      "-rw-rw-r-- 1 lidar lidar 178441413 May 11  2022 swin_tiny_patch4_window7_224_22k.pth\n",
      "drwxr-xr-x 3 root  root       4096 Apr 23 19:43 train\n",
      "drwxr-xr-x 2 root  root       4096 Apr 23 17:09 trt_inference\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_PROJECT_DIR/mask2former"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide experiment spec file <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "We provide a specification file to configure the key parameters for this demo including:\n",
    "\n",
    "* experiment config: configure the global experiment settings\n",
    "    * num_nodes: number of nodes (num_nodes=1 for single node)\n",
    "    * results_dir: the directory where your checkpoints will be saved\n",
    "    * checkpoint: pretrained weights (can be either a pretrained backbone model or a trained Mask2former model)\n",
    "* data config: configure the training and validation data\n",
    "    * train: training data config. required to be in COCO Panoptic format\n",
    "    * val: validation data config. required to be in COCO Panoptic format\n",
    "    * test: data config for test images\n",
    "* model config: configure the model setting\n",
    "    * backbone: the backbone config for Mask2former\n",
    "    * mode: prediction mode. Either \"panoptic\", \"semantic\" or \"instance\"\n",
    "* train_config: configure the training hyperparameters\n",
    "\n",
    "* **Note that the sample spec is not meant to produce SOTA accuracy on COCO. To reproduce SOTA, you might want to use TAO to train an ImageNet model first and follow the original parameters for COCO.**\n",
    "\n",
    "Please refer to the TAO documentation about Mask2former to get all the parameters that are configurable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_dir: /results_inst/\r\n",
      "dataset:\r\n",
      "  contiguous_id: True\r\n",
      "  label_map: /specs/labelmap_inst.json\r\n",
      "  train:\r\n",
      "    type: 'coco'\r\n",
      "    name: \"coco_2017_train\"\r\n",
      "    instance_json: \"/data/raw-data/annotations/instances_train2017.json\"\r\n",
      "    img_dir: \"/data/raw-data/train2017\"\r\n",
      "    batch_size: 16\r\n",
      "    num_workers: 2\r\n",
      "  val:\r\n",
      "    type: 'coco'\r\n",
      "    name: \"coco_2017_val\"\r\n",
      "    instance_json: \"/data/raw-data/annotations/instances_val2017.json\"\r\n",
      "    img_dir: \"/data/raw-data/val2017\"\r\n",
      "    batch_size: 1\r\n",
      "    num_workers: 2\r\n",
      "  test:\r\n",
      "    img_dir: /data/raw-data/val2017\r\n",
      "    batch_size: 1\r\n",
      "  augmentation:\r\n",
      "    train_min_size: [640]\r\n",
      "    train_max_size: 2048\r\n",
      "    train_crop_size: [640, 640]\r\n",
      "    test_min_size: 640\r\n",
      "    test_max_size: 2048\r\n",
      "train:\r\n",
      "  precision: 'fp16'\r\n",
      "  num_gpus: 1\r\n",
      "  checkpoint_interval: 1\r\n",
      "  validation_interval: 1\r\n",
      "  num_epochs: 50\r\n",
      "  optim:\r\n",
      "    lr_scheduler: \"MultiStep\"\r\n",
      "    milestones: [44, 48]\r\n",
      "    type: \"AdamW\"\r\n",
      "    lr: 0.0001\r\n",
      "    weight_decay: 0.05\r\n",
      "model:\r\n",
      "  object_mask_threshold: 0.1\r\n",
      "  overlap_threshold: 0.8\r\n",
      "  mode: \"instance\"\r\n",
      "  backbone:\r\n",
      "    pretrained_weights: \"/workspace/tao-experiments/mask2former/swin_tiny_patch4_window7_224_22k.pth\"\r\n",
      "    type: \"swin\"\r\n",
      "    swin:\r\n",
      "      type: \"tiny\"\r\n",
      "      window_size: 7\r\n",
      "      ape: False\r\n",
      "      pretrain_img_size: 224\r\n",
      "  mask_former:\r\n",
      "    num_object_queries: 100\r\n",
      "  sem_seg_head:\r\n",
      "    norm: \"GN\"\r\n",
      "    num_classes: 80\r\n",
      "export:\r\n",
      "  input_channel: 3\r\n",
      "  input_width: 640\r\n",
      "  input_height: 640\r\n",
      "  opset_version: 17\r\n",
      "  batch_size: -1  # dynamic batch size\r\n",
      "  on_cpu: False\r\n",
      "gen_trt_engine:\r\n",
      "  gpu_id: 0\r\n",
      "  input_channel: 3\r\n",
      "  input_width: 640\r\n",
      "  input_height: 640\r\n",
      "  tensorrt:\r\n",
      "    data_type: fp16\r\n",
      "    workspace_size: 4096\r\n",
      "    min_batch_size: 1\r\n",
      "    opt_batch_size: 1\r\n",
      "    max_batch_size: 1\r\n"
     ]
    }
   ],
   "source": [
    "# !cat $HOST_SPECS_DIR/spec_inst.yaml\n",
    "!cat $HOST_SPECS_DIR/spec.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* WARNING: COCO training takes about 40+ hours to complete using 8 V100 gpus. As a result, **we highly recommend that you run training with multiple high-end gpus (e.g. V100, A100)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=/data\n",
      "env: SPECS_DIR=/specs\n",
      "env: RESULTS_DIR=/results_inst\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker.\n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR=/data\n",
    "%env SPECS_DIR=/specs\n",
    "# %env RESULTS_DIR=/results_inst\n",
    "%env RESULTS_DIR=/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, set NUM_TRAIN_GPUS based on your machine.\")\n",
    "os.environ[\"NUM_TRAIN_GPUS\"] = \"1\"\n",
    "# !tao model mask2former train -e $SPECS_DIR/spec_inst.yaml \\\n",
    "#            train.num_gpus=$NUM_TRAIN_GPUS \\\n",
    "#            results_dir=$RESULTS_DIR\n",
    "!tao model mask2former train -e $SPECS_DIR/spec.yaml \\\n",
    "           train.num_gpus=$NUM_TRAIN_GPUS \\\n",
    "           results_dir=$RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model checkpoints:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set NUM_EPOCH to the epoch corresponding to any saved checkpoint\n",
    "# %env NUM_EPOCH=029\n",
    "\n",
    "# Get the name of the checkpoint corresponding to your set epoch\n",
    "# tmp=!ls $HOST_RESULTS_DIR/train/*.pth | grep epoch_$NUM_EPOCH\n",
    "# %env CHECKPOINT={tmp[0]}\n",
    "\n",
    "# Or get the latest checkpoint\n",
    "os.environ[\"CHECKPOINT\"] = os.path.join(os.getenv(\"HOST_RESULTS_DIR\"), \"train/mask2former_model_latest.pth\")\n",
    "\n",
    "print('Rename a trained model: ')\n",
    "print('---------------------')\n",
    "!cp $CHECKPOINT $HOST_RESULTS_DIR/train/mask2former_model.pth\n",
    "!ls -ltrh $HOST_RESULTS_DIR/train/mask2former_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate a trained model <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In this section, we run the `evaluate` tool to evaluate the trained model and produce the mIOU metric.\n",
    "\n",
    "In `spec_inst.yaml`, we specify a few key parameters for evaluation including:\n",
    "* model config: configure the model setting\n",
    "    * backbone: the backbone architecture for Mask2former\n",
    "* dataset config: configure the training and validation datasets\n",
    "    * val.img_dir: the root directory for validation images\n",
    "    * val.instance_json: annotation file for validation data. required to be in COCO Panoptic format\n",
    "* model config: configure the model setting\n",
    "    * arch: the backbone architecture for Mask2former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on TAO model\n",
    "!tao model mask2former evaluate -e $SPECS_DIR/spec_inst.yaml evaluate.checkpoint=$RESULTS_DIR/train/mask2former_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Inference <a class=\"anchor\" id=\"head-6\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models and visualize the results. The `inference` tool produces an output annotation json file with pseudo-mask info.\n",
    "\n",
    "In `spec_inst.yaml`, we specify a few key parameters for inference including:\n",
    "* model config: configure the model setting\n",
    "    * backbone: the backbone architecture for Mask2former\n",
    "* dataset config: configure the training and validation datasets\n",
    "    * test.img_dir: the root directory for validation images\n",
    "    * test.batch_size: batch_size of input images\n",
    "* model config: configure the model setting\n",
    "    * arch: the backbone architecture for Mask2former\n",
    "* augmentation config: configure the data preprocessing and augmentation\n",
    "    * test_min_size: minimum length of input's height or width\n",
    "    * test_max_size: maximum length of input's height or width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao model mask2former inference -e $SPECS_DIR/spec_inst.yaml inference.checkpoint=$RESULTS_DIR/train/mask2former_model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Visualize the result <a class=\"anchor\" id=\"head-6-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install deps\n",
    "!pip3 install Cython==0.29.36\n",
    "!pip3 install numpy\n",
    "!pip3 install pillow\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "!pip3 install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"inference\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "# Simple grid visualizer\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg']\n",
    "\n",
    "def visualize_images(output_path, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) \n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export the RGB model to ONNX model\n",
    "!tao model mask2former export \\\n",
    "           -e $SPECS_DIR/spec_inst.yaml \\\n",
    "           export.checkpoint=$RESULTS_DIR/train/mask2former_model.pth \\\n",
    "           export.onnx_file=$RESULTS_DIR/export/mask2former_model.onnx \\\n",
    "           results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate TensorRT engine\n",
    "!tao deploy mask2former gen_trt_engine \\\n",
    "           -e $SPECS_DIR/spec_inst.yaml \\\n",
    "           gen_trt_engine.onnx_file=$RESULTS_DIR/export/mask2former_model.onnx \\\n",
    "           gen_trt_engine.trt_engine=$RESULTS_DIR/gen_trt_engine/mask2former_model.engine \\\n",
    "           results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with generated TensorRT engine\n",
    "!tao deploy mask2former inference \\\n",
    "        -e $SPECS_DIR/spec_inst.yaml \\\n",
    "        inference.trt_engine=$RESULTS_DIR/gen_trt_engine/mask2former_model.engine \\\n",
    "        results_dir=$RESULTS_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "IMAGE_DIR = os.path.join(os.environ['HOST_RESULTS_DIR'], \"trt_inference\")\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "# Simple grid visualizer\n",
    "!pip3 install \"matplotlib>=3.3.3, <4.0\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg']\n",
    "\n",
    "def visualize_images(output_path, num_cols=4, num_images=10):\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) \n",
    "\n",
    "visualize_images(IMAGE_DIR, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
